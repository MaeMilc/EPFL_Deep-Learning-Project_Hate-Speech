{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Dataset Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LibreTranslate locally.\n",
    "\n",
    "Method 1 (Docker):\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/LibreTranslate/LibreTranslate\n",
    "./run.sh [args]\n",
    "```\n",
    "\n",
    "Method 2 (pip):\n",
    "\n",
    "```bash\n",
    "pip install libretranslate\n",
    "libretranslate [args]\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "- For both methods use args: ```--load-only en,ar ```\n",
    "- For Mac: **Disable AirPlay Receiver!** (it runs on port 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، كيف حالك؟?\n"
     ]
    }
   ],
   "source": [
    "# Test if server works\n",
    "\n",
    "url = \"http://127.0.0.1:5000/translate\"\n",
    "\n",
    "params = {\n",
    "    \"q\": \"Hello, how are you?\",\n",
    "    \"source\": \"en\",\n",
    "    \"target\": \"ar\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=params)\n",
    "    if response.status_code == 200:\n",
    "        translated_text = response.json()[\"translatedText\"]\n",
    "        print(translated_text)\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load, translate, and save datasets\n",
    "\n",
    "def translate_text(text, src_language, trg_language):\n",
    "    url = \"http://127.0.0.1:5000/translate\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"q\": text,\n",
    "        \"source\": src_language,\n",
    "        \"target\": trg_language,\n",
    "        \"format\": \"text\",\n",
    "        \"api_key\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"translatedText\"]\n",
    "        else:\n",
    "            print(f\"HTTP error: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error: {e}\")\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Ensure the line is not empty\n",
    "                parts = line.strip().split('\\t')\n",
    "                data.append(parts)\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def process_and_translate_dataset(file_path, output_file):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"Translating {file_path} . . .\")\n",
    "            \n",
    "    for row in tqdm(data, desc=\"Translating tweets\"):\n",
    "        tweet_id, tweet_text, off_label, hs_label, vulgar_label, violence_label = row\n",
    "        # Remove all emojis from the tweet text\n",
    "        tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "        # Replace <LF> with .\n",
    "        tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "        translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "        translated_data.append([tweet_id, translated_text, off_label, hs_label, vulgar_label, violence_label])\n",
    "        # print(f\"Tweet {tweet_id} done.\")\n",
    "    \n",
    "    print(f\"File {file_path} done.\")\n",
    "    \n",
    "    save_to_csv(translated_data, output_file)\n",
    "    \n",
    "def process_and_translate_dataset_test(file_path, output_file):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"Translating {file_path} . . .\")\n",
    "            \n",
    "    for row in tqdm(data, desc=\"Translating tweets\"):\n",
    "        tweet_id, tweet_text = row\n",
    "        # Remove all emojis from the tweet text\n",
    "        tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "        # Replace <LF> with .\n",
    "        tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "        translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "        translated_data.append([tweet_id, translated_text])\n",
    "        # print(f\"Tweet {tweet_id} done.\")\n",
    "    \n",
    "    print(f\"File {file_path} done.\")\n",
    "    \n",
    "    save_to_csv(translated_data, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحباً كيف حالك؟?\n",
      "Hi. How are you?\n",
      "['مرحبا، كيف حالك؟?', 'ما اسمك؟?']\n"
     ]
    }
   ],
   "source": [
    "# Test translate function\n",
    "\n",
    "msg_en = \"Hello. How are 😂😂😝😉 you?\"\n",
    "msg_ar = \"مرحباً كيف حالك؟?\"\n",
    "\n",
    "print(translate_text(msg_en, \"en\", \"ar\"))\n",
    "print(translate_text(msg_ar, \"ar\", \"en\"))\n",
    "\n",
    "# Batched\n",
    "msgs = [\"Hello, how are you?\", \"What is your name?\"]\n",
    "\n",
    "print(translate_text(msgs, \"en\", \"ar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"arabic-data/OSACT2022-sharedTask-dev.txt\",\n",
    "    \"arabic-data/OSACT2022-sharedTask-test-tweets.txt\",\n",
    "    \"arabic-data/OSACT2022-sharedTask-train.txt\"\n",
    "]\n",
    "\n",
    "output_files = [\n",
    "    \"translated-data/OSACT2022-sharedTask-dev-en.csv\",\n",
    "    \"translated-data/OSACT2022-sharedTask-test-tweets-en.csv\",\n",
    "    \"translated-data/OSACT2022-sharedTask-train-en.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before replacement: RT @USER ابديت السناب  الجديد ❌<LF>حاس الناس حوس ،أشوف مشاهير. تضيفني، مشاهير تتابع يومياتي ، ابديت كرهني بالسناب  كله 😤\n",
      "After replacement: RT @USER ابديت السناب  الجديد .حاس الناس حوس ،أشوف مشاهير. تضيفني، مشاهير تتابع يومياتي ، ابديت كرهني بالسناب  كله \n",
      "After transalation: RT @USER ابديت السناب  الجديد .حاس الناس حوس ،أشوف مشاهير. تضيفني، مشاهير تتابع يومياتي ، ابديت كرهني بالسناب  كله \n"
     ]
    }
   ],
   "source": [
    "# Test process_and_translate_dataset function for single rows in dataset\n",
    "\n",
    "# Regex pattern to match emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def process_and_translate_dataset_test(file_path, output_file, row_id):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    # for row in data:\n",
    "    row = data[row_id]\n",
    "    tweet_id, tweet_text, off_label, hs_label, vulgar_label, violence_label = row\n",
    "    print(f\"Before replacement: {tweet_text}\")\n",
    "    # Remove all emojis from the tweet text\n",
    "    tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "    # Replace <LF> with .\n",
    "    tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "    print(f\"After replacement: {tweet_text}\")\n",
    "    translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "    translated_data.append([tweet_id, translated_text, off_label, hs_label, vulgar_label, violence_label])\n",
    "    print(f\"After transalation: {tweet_text}\")\n",
    "    save_to_csv(translated_data, output_file)\n",
    "\n",
    "row_id = 2\n",
    "process_and_translate_dataset_test('arabic-data/OSACT2022-sharedTask-dev.txt', f'translated-data/test{row_id}', row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating arabic-data/OSACT2022-sharedTask-dev.txt . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10002eff05d24e6d9efea40ce54c9905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating tweets:   0%|          | 0/1270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File arabic-data/OSACT2022-sharedTask-dev.txt done.\n",
      "Translating arabic-data/OSACT2022-sharedTask-test-tweets.txt . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f6945a0524452c90139f30a3cdaefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating tweets:   0%|          | 0/2541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mprocess_and_translate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 45\u001b[0m, in \u001b[0;36mprocess_and_translate_dataset\u001b[0;34m(file_path, output_file)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m . . .\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslating tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 45\u001b[0m     tweet_id, tweet_text, off_label, hs_label, vulgar_label, violence_label \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Remove all emojis from the tweet text\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     tweet_text \u001b[38;5;241m=\u001b[39m emoji_pattern\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, tweet_text)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 2)"
     ]
    }
   ],
   "source": [
    "process_and_translate_dataset(file_paths[0], output_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating arabic-data/OSACT2022-sharedTask-train.txt . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad06176bf3224b16a0db1008f7b6488d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating tweets:   0%|          | 0/8887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File arabic-data/OSACT2022-sharedTask-train.txt done.\n",
      "Translating arabic-data/OSACT2022-sharedTask-test-tweets.txt . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178cf786346c42bcabf7a9e70ea93b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating tweets:   0%|          | 0/2541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File arabic-data/OSACT2022-sharedTask-test-tweets.txt done.\n"
     ]
    }
   ],
   "source": [
    "process_and_translate_dataset(file_paths[2], output_files[2])\n",
    "process_and_translate_dataset_test(file_paths[1], output_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
