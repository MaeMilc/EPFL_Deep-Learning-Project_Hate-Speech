{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Dataset Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romino/Documents-local/projects/env-dl/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LibreTranslate locally.\n",
    "\n",
    "Method 1 (Docker):\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/LibreTranslate/LibreTranslate\n",
    "./run.sh [args]\n",
    "```\n",
    "\n",
    "Method 2 (pip):\n",
    "\n",
    "```bash\n",
    "pip install libretranslate\n",
    "libretranslate [args]\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "- For both methods use args: ```--load-only en,ar ```\n",
    "- For Mac: **Disable AirPlay Receiver!** (it runs on port 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ?\n"
     ]
    }
   ],
   "source": [
    "# Test if server works\n",
    "\n",
    "url = \"http://127.0.0.1:5000/translate\"\n",
    "\n",
    "params = {\n",
    "    \"q\": \"Hello, how are you?\",\n",
    "    \"source\": \"en\",\n",
    "    \"target\": \"ar\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=params)\n",
    "    if response.status_code == 200:\n",
    "        translated_text = response.json()[\"translatedText\"]\n",
    "        print(translated_text)\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load, translate, and save datasets\n",
    "\n",
    "def translate_text(text, src_language, trg_language):\n",
    "    url = \"http://127.0.0.1:5000/translate\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"q\": text,\n",
    "        \"source\": src_language,\n",
    "        \"target\": trg_language,\n",
    "        \"format\": \"text\",\n",
    "        \"api_key\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"translatedText\"]\n",
    "        else:\n",
    "            print(f\"HTTP error: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error: {e}\")\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Ensure the line is not empty\n",
    "                parts = line.strip().split('\\t')\n",
    "                data.append(parts)\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def process_and_translate_dataset(file_path, output_file):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"Translating {file_path} . . .\")\n",
    "            \n",
    "    for row in tqdm(data, desc=\"Translating tweets\"):\n",
    "        tweet_id, tweet_text, off_label, hs_label, vulgar_label, violence_label = row\n",
    "        # Remove all emojis from the tweet text\n",
    "        tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "        # Replace <LF> with .\n",
    "        tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "        translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "        translated_data.append([tweet_id, translated_text, off_label, hs_label, vulgar_label, violence_label])\n",
    "        # print(f\"Tweet {tweet_id} done.\")\n",
    "    \n",
    "    print(f\"File {file_path} done.\")\n",
    "    \n",
    "    save_to_csv(translated_data, output_file)\n",
    "    \n",
    "def process_and_translate_dataset_test(file_path, output_file):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"Translating {file_path} . . .\")\n",
    "            \n",
    "    for row in tqdm(data, desc=\"Translating tweets\"):\n",
    "        tweet_id, tweet_text = row\n",
    "        # Remove all emojis from the tweet text\n",
    "        tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "        # Replace <LF> with .\n",
    "        tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "        translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "        translated_data.append([tweet_id, translated_text])\n",
    "        # print(f\"Tweet {tweet_id} done.\")\n",
    "    \n",
    "    print(f\"File {file_path} done.\")\n",
    "    \n",
    "    save_to_csv(translated_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ?\n",
      "Hi. How are you?\n",
      "['Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ?', 'Ù…Ø§ Ø§Ø³Ù…ÙƒØŸ?']\n"
     ]
    }
   ],
   "source": [
    "# Test translate function\n",
    "\n",
    "msg_en = \"Hello. How are ğŸ˜‚ğŸ˜‚ğŸ˜ğŸ˜‰ you?\"\n",
    "msg_ar = \"Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ?\"\n",
    "\n",
    "print(translate_text(msg_en, \"en\", \"ar\"))\n",
    "print(translate_text(msg_ar, \"ar\", \"en\"))\n",
    "\n",
    "# Batched\n",
    "msgs = [\"Hello, how are you?\", \"What is your name?\"]\n",
    "\n",
    "print(translate_text(msgs, \"en\", \"ar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"arabic-data/OSACT2022-sharedTask-dev.txt\",\n",
    "    \"arabic-data/OSACT2022-sharedTask-test-tweets.txt\",\n",
    "    \"arabic-data/OSACT2022-sharedTask-train.txt\"\n",
    "]\n",
    "\n",
    "output_files = [\n",
    "    \"translated-data/OSACT2022-sharedTask-dev-en.csv\",\n",
    "    \"translated-data/OSACT2022-sharedTask-test-tweets-en.csv\",\n",
    "    \"translated-data/OSACT2022-sharedTask-train-en.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before replacement: RT @USER Ø§Ø¨Ø¯ÙŠØª Ø§Ù„Ø³Ù†Ø§Ø¨  Ø§Ù„Ø¬Ø¯ÙŠØ¯ âŒ<LF>Ø­Ø§Ø³ Ø§Ù„Ù†Ø§Ø³ Ø­ÙˆØ³ ØŒØ£Ø´ÙˆÙ Ù…Ø´Ø§Ù‡ÙŠØ±. ØªØ¶ÙŠÙÙ†ÙŠØŒ Ù…Ø´Ø§Ù‡ÙŠØ± ØªØªØ§Ø¨Ø¹ ÙŠÙˆÙ…ÙŠØ§ØªÙŠ ØŒ Ø§Ø¨Ø¯ÙŠØª ÙƒØ±Ù‡Ù†ÙŠ Ø¨Ø§Ù„Ø³Ù†Ø§Ø¨  ÙƒÙ„Ù‡ ğŸ˜¤\n",
      "After replacement: RT @USER Ø§Ø¨Ø¯ÙŠØª Ø§Ù„Ø³Ù†Ø§Ø¨  Ø§Ù„Ø¬Ø¯ÙŠØ¯ .Ø­Ø§Ø³ Ø§Ù„Ù†Ø§Ø³ Ø­ÙˆØ³ ØŒØ£Ø´ÙˆÙ Ù…Ø´Ø§Ù‡ÙŠØ±. ØªØ¶ÙŠÙÙ†ÙŠØŒ Ù…Ø´Ø§Ù‡ÙŠØ± ØªØªØ§Ø¨Ø¹ ÙŠÙˆÙ…ÙŠØ§ØªÙŠ ØŒ Ø§Ø¨Ø¯ÙŠØª ÙƒØ±Ù‡Ù†ÙŠ Ø¨Ø§Ù„Ø³Ù†Ø§Ø¨  ÙƒÙ„Ù‡ \n",
      "After transalation: RT @USER Ø§Ø¨Ø¯ÙŠØª Ø§Ù„Ø³Ù†Ø§Ø¨  Ø§Ù„Ø¬Ø¯ÙŠØ¯ .Ø­Ø§Ø³ Ø§Ù„Ù†Ø§Ø³ Ø­ÙˆØ³ ØŒØ£Ø´ÙˆÙ Ù…Ø´Ø§Ù‡ÙŠØ±. ØªØ¶ÙŠÙÙ†ÙŠØŒ Ù…Ø´Ø§Ù‡ÙŠØ± ØªØªØ§Ø¨Ø¹ ÙŠÙˆÙ…ÙŠØ§ØªÙŠ ØŒ Ø§Ø¨Ø¯ÙŠØª ÙƒØ±Ù‡Ù†ÙŠ Ø¨Ø§Ù„Ø³Ù†Ø§Ø¨  ÙƒÙ„Ù‡ \n"
     ]
    }
   ],
   "source": [
    "# Test process_and_translate_dataset function for single rows in dataset\n",
    "\n",
    "# Regex pattern to match emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def process_and_translate_dataset_test(file_path, output_file, row_id):\n",
    "    data = read_data(file_path)\n",
    "    translated_data = []\n",
    "    \n",
    "    # for row in data:\n",
    "    row = data[row_id]\n",
    "    tweet_id, tweet_text, off_label, hs_label, vulgar_label, violence_label = row\n",
    "    print(f\"Before replacement: {tweet_text}\")\n",
    "    # Remove all emojis from the tweet text\n",
    "    tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "    # Replace <LF> with .\n",
    "    tweet_text = tweet_text.replace(\"<LF>\", \".\")\n",
    "    print(f\"After replacement: {tweet_text}\")\n",
    "    translated_text = translate_text(tweet_text, 'ar', 'en')\n",
    "    translated_data.append([tweet_id, translated_text, off_label, hs_label, vulgar_label, violence_label])\n",
    "    print(f\"After transalation: {tweet_text}\")\n",
    "    save_to_csv(translated_data, output_file)\n",
    "\n",
    "row_id = 2\n",
    "process_and_translate_dataset_test('arabic-data/OSACT2022-sharedTask-dev.txt', f'translated-data/test{row_id}', row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_translate_dataset(file_paths[0], output_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_translate_dataset(file_paths[2], output_files[2])\n",
    "process_and_translate_dataset_test(file_paths[1], output_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
